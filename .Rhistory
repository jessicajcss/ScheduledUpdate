by = "hour"))
meteo_rbs_dc <- dplyr::right_join(meteo_rbs_dc, temp, by = "timestamp")
res2 <- dplyr::right_join(res2, temp, by = "timestamp")
# substituindo periodo dados NA estação defesa civil por dados escola
meteo_rbs <- dplyr::rows_patch(meteo_rbs_dc, res2, by = "timestamp")
summary(meteo_rbs) # valores dentro do "normal"
#selecionando dados existentes
meteo_rbs <- meteo_rbs[!is.na(meteo_rbs$deltat), ]
# selecionando variáveis de interesse
meteo_rbs <- meteo_rbs |>
dplyr::mutate(Cidade = "Rio Branco do Sul") |>
dplyr::select(Cidade, timestamp, temp, wind, direction, rain, humidity, radiation, pressure, uv)
# Function to get JWT token (only when needed)
get_jwt_token <- function() {
if (exists("global_jwt_token") && !is.null(global_jwt_token)) {
return(global_jwt_token)
}
plugfield_auth <- Sys.getenv("PLUG_AUTH")  # Simula a requisição do token, new Auth using POSTMAN
global_jwt_token <<- plugfield_auth  # Save token globally
return(global_jwt_token)
}
jwt_token <- NULL  # Variável global
get_jwt_token <- function() {
if (!is.null(jwt_token)) {
return(jwt_token)
}
plugfield_auth <- Sys.getenv("PLUG_AUTH")  # Simula requisição
assign("jwt_token", plugfield_auth, envir = .GlobalEnv)  # Salva na memória
return(plugfield_auth)
}
# Function to fetch data (optimizing request frequency)
fetch_large_data <- function(device_id, start_date, end_date) {
max_days <- 30  # API limit
all_data <- list()  # Store all chunks
current_start <- start_date
plugfield_api <- Sys.getenv("PLUG_API")
jwt_token <- get_jwt_token()  # Fetch token once
while (current_start < end_date) {
current_end <- min(current_start + lubridate::days(max_days), end_date)  # 30-day chunks
# Format API request
begin_date <- format(current_start, "%d/%m/%Y+%H")
end_date_formatted <- format(current_end, "%d/%m/%Y+%H")
url <- paste0("https://prod-api.plugfield.com.br/data/hourly?device=", device_id,
"&begin=", begin_date,
"&end=", end_date_formatted)
# Debugging: Print the URL to check for correctness
print(paste("Requesting URL:", url))
headers <- httr::add_headers(
'accept' = 'application/json',
'x-api-key' = plugfield_api,
'Authorization' = jwt_token
)
# Request data
res <- httr::GET(url, headers)
if (httr::status_code(res) == 403) {
message("Token expired, requesting new token...")
jwt_token <<- get_jwt_token()  # Refresh token
headers <- httr::add_headers(
'accept' = 'application/json',
'x-api-key' = plugfield_api,
'Authorization' = jwt_token
)
res <- httr::GET(url, headers)  # Retry request
}
# Process response
if (httr::status_code(res) == 200) {
json_data <- jsonlite::fromJSON(httr::content(res, "text", encoding = "UTF-8"), flatten = TRUE)
df <- as.data.frame(json_data)
all_data <- append(all_data, list(df))
} else {
message("Error:", httr::status_code(res))
}
# Move to next period
current_start <- current_end + lubridate::seconds(1)
Sys.sleep(0.2)  # Avoid excessive requests
}
return(dplyr::bind_rows(all_data))
}
load(file = "./data/meteo/meteo_rbs.Rda")
View(meteo_rbs)
last_meteo_rbs <- meteo_rbs |>
dplyr::mutate(date = lubridate::force_tz(date, tz = "America/Sao_Paulo"))
rm(meteo_rbs)
ultima_data <- last_meteo_rbs |>
dplyr::mutate(date = lubridate::as_datetime(date, tz = "America/Sao_Paulo")) |>
dplyr::arrange(date) |>
tail(1) |>
dplyr::select(date)
#########################################################################
start_date <- ultima_data$date #"2023-06-01" # máximo de um ano!!
#start_date <- ymd_hms("2025-01-01 00:00:00")
end_date <- Sys.time() #"26/02/2025 00:00:00"  # End period (more than 30 days)
# DADOS ESCOLA
device_id <- 3184
df_escola <- fetch_large_data(device_id, start_date, end_date)
View(df_escola)
# DADOS DEFESA CIVIL
device_id <- 3118
end_date <- Sys.time() #"26/02/2025 00:00:00"  # End period (more than 30 days)
df_defesacivil <- fetch_large_data(device_id, start_date, end_date)
# joining all data from rio branco do sul
df <- rbind(df_escola, df_defesacivil)
# to remove columns where all values are either NA or 0 from your dataframe df:
# Print first rows
head(df)
df <- df |>
dplyr::mutate(across(where(is.list), ~ purrr::map_chr(., ~ if (length(.) == 0) NA else paste(., collapse = ",")))) |>  # Flatten lists
dplyr::select(where(~ !all(is.na(.))) & where(~ !all(. == 0, na.rm = TRUE)) & where(~ !all(. == "NULL", na.rm = TRUE)))  # Remove all-NA or all-0 columns
head(df)  # Check the cleaned dataframe
# fix timestamp format
df <- df |>
dplyr::mutate(timestamp = lubridate::ymd_hms(localDateTime))
head(df$timestamp)
summary(df)
# Preparando periodo sem dados na estação defesa civil [NA] para substituir por dados escola
meteo_rbs_dc <- df |>
subset(deviceId == 3118)
meteo_rbs_esc <- df |>
subset(deviceId == 3184)
res <- meteo_rbs_dc[is.na(meteo_rbs_dc$tempMax), ] # dados inválidos DC
if (nrow(res) > 0) {
res[,c(1:37)] <- NA
res1 <- meteo_rbs_dc[!is.na(meteo_rbs_dc$tempMax), ] # dados válidos DC
res2 <- meteo_rbs_esc[!is.na(meteo_rbs_esc$tempMax), ] |>
unique() # dados válidos ESC
meteo_rbs_dc <- rbind(res1, res) |>
dplyr::arrange(timestamp) |>
unique()
}
temp <- data.frame(timestamp = seq(
from = lubridate::as_datetime(start_date),
to = lubridate::as_datetime(end_date),
by = "hour"))
meteo_rbs_dc <- dplyr::right_join(meteo_rbs_dc, temp, by = "timestamp")
res2 <- dplyr::right_join(res2, temp, by = "timestamp")
# substituindo periodo dados NA estação defesa civil por dados escola
meteo_rbs <- dplyr::rows_patch(meteo_rbs_dc, res2, by = "timestamp")
summary(meteo_rbs) # valores dentro do "normal"
#selecionando dados existentes
meteo_rbs <- meteo_rbs[!is.na(meteo_rbs$deltat), ]
# selecionando variáveis de interesse
meteo_rbs <- meteo_rbs |>
dplyr::mutate(Cidade = "Rio Branco do Sul") |>
dplyr::select(Cidade, timestamp, temp, wind, direction, rain, humidity, radiation, pressure, uv)
View(df_escola)
View(meteo_rbs_dc)
View(df)
# joining all data from rio branco do sul
df <- rbind(df_escola, df_defesacivil)
# to remove columns where all values are either NA or 0 from your dataframe df:
# Print first rows
head(df)
df <- df |>
dplyr::mutate(across(where(is.list), ~ purrr::map_chr(., ~ if (length(.) == 0) NA else paste(., collapse = ",")))) |>  # Flatten lists
dplyr::select(where(~ !all(is.na(.))) &  where(~ !all(. == "NULL", na.rm = TRUE)))  # Remove all-NA or all-NULL columns [where(~ !all(. == 0, na.rm = TRUE)) &]
head(df)  # Check the cleaned dataframe
# fix timestamp format
df <- df |>
dplyr::mutate(timestamp = lubridate::ymd_hms(localDateTime))
head(df$timestamp)
summary(df)
# Preparando periodo sem dados na estação defesa civil [NA] para substituir por dados escola
meteo_rbs_dc <- df |>
subset(deviceId == 3118)
meteo_rbs_esc <- df |>
subset(deviceId == 3184)
res <- meteo_rbs_dc[is.na(meteo_rbs_dc$tempMax), ] # dados inválidos DC
if (nrow(res) > 0) {
res[,c(1:37)] <- NA
res1 <- meteo_rbs_dc[!is.na(meteo_rbs_dc$tempMax), ] # dados válidos DC
res2 <- meteo_rbs_esc[!is.na(meteo_rbs_esc$tempMax), ] |>
unique() # dados válidos ESC
meteo_rbs_dc <- rbind(res1, res) |>
dplyr::arrange(timestamp) |>
unique()
}
temp <- data.frame(timestamp = seq(
from = lubridate::as_datetime(start_date),
to = lubridate::as_datetime(end_date),
by = "hour"))
meteo_rbs_dc <- dplyr::right_join(meteo_rbs_dc, temp, by = "timestamp")
res2 <- dplyr::right_join(res2, temp, by = "timestamp")
# substituindo periodo dados NA estação defesa civil por dados escola
meteo_rbs <- dplyr::rows_patch(meteo_rbs_dc, res2, by = "timestamp")
summary(meteo_rbs) # valores dentro do "normal"
#selecionando dados existentes
meteo_rbs <- meteo_rbs[!is.na(meteo_rbs$deltat), ]
# selecionando variáveis de interesse
meteo_rbs <- meteo_rbs |>
dplyr::mutate(Cidade = "Rio Branco do Sul") |>
dplyr::select(Cidade, timestamp, temp, wind, direction, rain, humidity, radiation, pressure, uv)
colnames(meteo_rbs) <- c('Cidade', 'date',
'temp', 'ws', 'wd', 'prec', 'umid', 'rad', 'press', 'uv')
View(meteo_rbs)
View(last_meteo_rbs)
View(df_escola)
# joining all data from rio branco do sul
df <- rbind(df_escola, df_defesacivil)
# to remove columns where all values are either NA or 0 from your dataframe df:
# Print first rows
head(df)
df <- df |>
dplyr::mutate(across(where(is.list), ~ purrr::map_chr(., ~ if (length(.) == 0) NA else paste(., collapse = ",")))) |>  # Flatten lists
dplyr::select(where(~ !all(is.na(.))) &  where(~ !all(. == "NULL", na.rm = TRUE)))  # Remove all-NA or all-NULL columns [where(~ !all(. == 0, na.rm = TRUE)) &]
head(df)  # Check the cleaned dataframe
# fix timestamp format
df <- df |>
dplyr::mutate(timestamp = lubridate::ymd_hms(localDateTime))
head(df$timestamp)
summary(df)
# Preparando periodo sem dados na estação defesa civil [NA] para substituir por dados escola
meteo_rbs_dc <- df |>
subset(deviceId == 3118)
meteo_rbs_esc <- df |>
subset(deviceId == 3184)
res <- meteo_rbs_dc[is.na(meteo_rbs_dc$tempMax), ] # dados inválidos DC
source("R/00-access-purpleair-data.R")
source("R/00-access-INMET_API-data.R")
source("R/01-meteo_hour.R")
source("R/00-access-REPOnewdata-thermo-git.R")
source("R/00-data_wrangling_thermo.R")
source("R/01-AQI_calculation_thermo_data.R")
#https://www.r-bloggers.com/2020/09/running-an-r-script-on-a-schedule-gh-actions/
renv::init()
renv::status()
renv::snapshot()
conflicted::conflict_scout(
source("./R/00-access-newdata-thermo-git.R")
)
conflicted::conflict_scout(
source("./R/00-access-REPOnewdata-thermo-git.R")
)
conflicted::conflict_scout(
source("./R/00-access-REPOnewdata-thermo-git.R")
)
# Define repositories to fetch (replace with your list of repositories)
thermo_orgs <- c("Dados_GM_UFPR")  # Replace with actual repo names
github_pat <- Sys.getenv("GITHUB_PAT")
# Function to fetch the trees for each repository
get_repo_trees <- function(repo_name) {
# GitHub API endpoint to get the repository's tree
cat("Fetching tree for repo:", repo_name, "\n")
response <- gh::gh(
"GET /repos/{owner}/{repo}/git/trees/{tree_sha}",
owner = "jessicajcss",  # Replace with the owner of the repository
repo = repo_name,
tree_sha = "main",  # Replace with the tree SHA you want to retrieve, e.g., 'main' or a specific SHA
.token = github_pat,  # Use the GitHub PAT from the environment
.accept = "application/vnd.github.v3.raw",
recursive = F
)
cat("Response received for repo:", repo_name, "\n")
return(response)
}
# Iterate over all repositories and fetch the trees
thermo_repos_raw <- purrr::map(thermo_orgs, function(repo) {
tryCatch(
{
response <- get_repo_trees(repo)
cat("Successfully fetched tree for repo:", repo, "\n")
return(response)
},
error = function(e) {
cat("Error fetching tree for repo:", repo, "\n", "Error message:", e$message, "\n")
return(NULL)
}
)
})
# Print the result
print(thermo_repos_raw)
thermo_repos <- thermo_repos_raw[[1]]$tree |>
#purrr::flatten() |>
purrr::map(unlist, recursive = TRUE)  |>
purrr::map_dfr(tibble::enframe, .id = "id_repo") |>
tidyr::pivot_wider() |>
dplyr::filter(stringr::str_detect(path,'.lsi')) |>
tidyr::separate(path, c('folder','filename'),'/')
load("./data_raw/file_path.Rda") # data previously downloaded
url <- thermo_repos$url
file_path_new <- url |>
as.data.frame()
result <- thermo_repos |>
dplyr::anti_join(file_path, by = "url")
# paths to save unique url from files already downloaded to exclude them from the next loop
file_path <- file_path_new
i <- 1
out <- vector("list", nrow(result)) # vetor com o correspondente numero de variaveis meteorologicas
for (i in seq(i, nrow(result))){
result$filename[i] <- sub(" ", "%20", result$filename[i])
path <- paste0("https://raw.githubusercontent.com/jessicajcss/Dados_GM_UFPR/refs/heads/main/", result$folder[i], "/", result$filename[i])
response <- httr::GET(path,
httr::authenticate(Sys.getenv("GITHUB_PAT"), ""),
Accept = "application/vnd.github.v3.raw")
file_content <- readLines(textConnection(httr::content(response, as = "text")))
out[[i]]$Cidade <- result$folder[i]
out[[i]]$value <- file_content[1]
print(path)
Sys.sleep(0.000001)
}
result$filename[i] <- sub(" ", "%20", result$filename[i])
path <- paste0("https://raw.githubusercontent.com/jessicajcss/Dados_GM_UFPR/refs/heads/main/", result$folder[i], "/", result$filename[i])
response <- httr::GET(path,
httr::authenticate(Sys.getenv("GITHUB_PAT"), ""),
Accept = "application/vnd.github.v3.raw")
file_content <- readLines(textConnection(httr::content(response, as = "text")))
out[[i]]$Cidade <- result$folder[i]
file_content <- readLines(textConnection(httr::content(response, as = "text")))
out[[i]]$Cidade <- result$folder[i]
result
nrow(result)
conflicted::conflict_scout(
source("./R/00-access-REPOnewdata-thermo-git.R")
)
nrow(result)
# Define repositories to fetch (replace with your list of repositories)
thermo_orgs <- c("Dados_GM_UFPR")  # Replace with actual repo names
github_pat <- Sys.getenv("GITHUB_PAT")
# Function to fetch the trees for each repository
get_repo_trees <- function(repo_name) {
# GitHub API endpoint to get the repository's tree
cat("Fetching tree for repo:", repo_name, "\n")
response <- gh::gh(
"GET /repos/{owner}/{repo}/git/trees/{tree_sha}",
owner = "jessicajcss",  # Replace with the owner of the repository
repo = repo_name,
tree_sha = "main",  # Replace with the tree SHA you want to retrieve, e.g., 'main' or a specific SHA
.token = github_pat,  # Use the GitHub PAT from the environment
.accept = "application/vnd.github.v3.raw",
recursive = F
)
cat("Response received for repo:", repo_name, "\n")
return(response)
}
# Iterate over all repositories and fetch the trees
thermo_repos_raw <- purrr::map(thermo_orgs, function(repo) {
tryCatch(
{
response <- get_repo_trees(repo)
cat("Successfully fetched tree for repo:", repo, "\n")
return(response)
},
error = function(e) {
cat("Error fetching tree for repo:", repo, "\n", "Error message:", e$message, "\n")
return(NULL)
}
)
})
# Print the result
print(thermo_repos_raw)
thermo_repos <- thermo_repos_raw[[1]]$tree |>
#purrr::flatten() |>
purrr::map(unlist, recursive = TRUE)  |>
purrr::map_dfr(tibble::enframe, .id = "id_repo") |>
tidyr::pivot_wider() |>
dplyr::filter(stringr::str_detect(path,'.lsi')) |>
tidyr::separate(path, c('folder','filename'),'/')
load("./data_raw/file_path.Rda") # data previously downloaded
url <- thermo_repos$url
file_path_new <- url |>
as.data.frame()
result <- thermo_repos |>
dplyr::anti_join(file_path, by = "url")
if (nrow(result) > 0) {
# paths to save unique url from files already downloaded to exclude them from the next loop
file_path <- file_path_new
#Looping and save file to local
##We`ll loop through all the paths and request the data by generating url for each file, converting datatype of all columns to character and appending all files to a consolidated one.
i <- 1
out <- vector("list", nrow(result)) # vetor com o correspondente numero de variaveis meteorologicas
for (i in seq(i, nrow(result))){
result$filename[i] <- sub(" ", "%20", result$filename[i])
path <- paste0("https://raw.githubusercontent.com/jessicajcss/Dados_GM_UFPR/refs/heads/main/", result$folder[i], "/", result$filename[i])
response <- httr::GET(path,
httr::authenticate(Sys.getenv("GITHUB_PAT"), ""),
Accept = "application/vnd.github.v3.raw")
file_content <- readLines(textConnection(httr::content(response, as = "text")))
out[[i]]$Cidade <- result$folder[i]
out[[i]]$value <- file_content[1]
print(path)
Sys.sleep(0.000001)
}
# Unificando o dataset
output <- dplyr::bind_rows(out) |>
dplyr::distinct() # combine the output into a single data frame
# Preparando o banco de dados
data_git <- output |>
tidyr::separate(col = value,
into = c("id", "date", "no2", "x4",
"o3", "x6", "so2", "x8",
"rh_sensor", "x10", "co", "x12",
"pm2p5", "x14", "pm10", "x16", "x17"),
sep = ",") |>
dplyr::mutate(date = as.POSIXct(date,
format = "%m/%d/%Y %I:%M:%S %p",
tz = 'America/Sao_Paulo')) |> #https://www.kaggle.com/discussions/questions-and-answers/382740
tidyr::drop_na() |>
dplyr::mutate(across(c(so2, no2, o3, co, pm2p5, pm10, rh_sensor), as.numeric)) |>
dplyr::select(Cidade, date, so2, no2, o3, co, pm2p5, pm10, rh_sensor) |>
dplyr::mutate(Cidade = dplyr::recode((Cidade),
"GM-RioBranco" = "Rio Branco do Sul", #02/08/23 15:50
"GM-AlmiranteTamandare" = "Almirante Tamandaré"))
colnames(data_git) <- c('Cidade','date','SO2', 'NO2', 'O3', 'CO', 'PM2.5','PM10', 'rh_sensor')
# Loading previous full dataset
load("./data/data_thermo_update.Rda")
data_thermo <- rbind(data_thermo, data_git) |>
unique() |>
dplyr::arrange(date)
} else {
data_thermo <- data_thermo
file_path <- file_path
}
# saving output dados até [22-02-2025 24h]
save(data_thermo, file="./data/data_thermo_update.Rda")
save(file_path, file="./data_raw/file_path.Rda")
conflicted::conflict_scout(
source("./R/00-access-REPOnewdata-thermo-git.R")
)
token <- seu_token
# Function to get INMET token from environment variables
get_inmet_token <- function() {
token <- seu_token
if (token == "") {
stop("The INMET API requires a token. Please set the INMET_TOKEN environment variable.")
}
return(token)
}
# Function to download and parse data from the INMET API
download_data <- function(start_date, end_date, station_code) {
token <- get_inmet_token()
url <- sprintf("https://apitempo.inmet.gov.br/token/estacao/%s/%s/%s/%s",
start_date, end_date, station_code, token)
response <- httr::GET(url)
# Check if the request was successful
if (httr::status_code(response) != 200) {
stop(paste("Error:", httr::status_code(response), "-", httr::http_status(response)$message))
}
content <- httr::content(response, "text", encoding = "UTF-8")
# Check if response is empty
if (nzchar(content) == FALSE) {
stop("Error: API response is empty.")
}
# Try to parse JSON safely
parsed_json <- tryCatch(
jsonlite::fromJSON(content),
error = function(e) {
stop("Error: Failed to parse JSON response. Check API request.")
}
)
return(parsed_json)
}
# Function to convert JSON data to a data frame
convert_to_df <- function(data) {
if (length(data) == 0) {
stop("Error: No data available for the given parameters.")
}
# Replace NULL values with NA
as_missing <- function(v) ifelse(is.null(v), NA, v)
df <- as.data.frame(lapply(data, function(col) sapply(col, as_missing)))
# Convert numeric columns to numeric types
num_cols <- c("VL_LONGITUDE", "VL_LATITUDE", "VL_ALTITUDE",
"TEM_INS", "TEM_MIN", "TEM_MAX",
"TEMP_MIN", "TEMP_MED", "TEMP_MAX",
"UMD_INS", "UMD_MIN", "UMD_MAX",
"UMID_MIN", "UMID_MED", "UMID_MAX",
"PRE_INS", "PRE_MIN", "PRE_MAX",
"VEN_VEL", "VEN_RAJ", "VEN_DIR",
"PTO_INS", "PTO_MIN", "PTO_MAX",
"RAD_GLO", "CHUVA")
for (col in num_cols) {
if (col %in% names(df)) {
df[[col]] <- as.numeric(df[[col]])
}
}
return(df)
}
load(file = "./data/meteo_colombo.Rda")
last_meteo_colombo <- meteo_colombo
rm(meteo_colombo)
ultima_data <- last_meteo_colombo |>
dplyr::mutate(date = as.Date(date)) |>
dplyr::arrange(date) |>
tail(1) |>
dplyr::select(date)
# Exampledate# Example usage ----
start_date <- ultima_data$date #"2023-06-01" # máximo de um ano!!
end_date <- Sys.Date()+1
station_code <- "B806"
seu_token <- Sys.getenv("INMET_TOKEN")
data <- download_data(start_date, end_date, station_code)
df <- convert_to_df(data)
# Print the first few rows
print(head(df))
meteo_colombo <- df |>
dplyr::mutate(data = as.Date(DT_MEDICAO),
time = sub("00", "", HR_MEDICAO),# sub("(\\d+)(\\d{2})", "\\1:\\2", Hora.Medicao))
date = lubridate::ymd_hms(paste0(data," ", time, ":00:00")),
uv = NA) |>
dplyr::select(DC_NOME, date, TEM_INS, VEN_VEL, VEN_DIR, CHUVA, UMD_INS, RAD_GLO, PRE_INS, uv)
colnames(meteo_colombo) <- c('Cidade', 'date', 'temp', 'ws', 'wd', 'prec', 'umid', 'rad', 'press', 'uv')
meteo_colombo <- meteo_colombo |>
dplyr::mutate(date = lubridate::with_tz(date, tz = "America/Chicago")) |>
dplyr::mutate(date = lubridate::force_tz(date, tz = "America/Sao_Paulo")) |>
dplyr::mutate(Cidade = "Colombo",
across(c(temp, ws, wd, prec, umid, rad, press, uv), as.numeric))
meteo_colombo <- rbind(meteo_colombo, last_meteo_colombo) |>
dplyr::arrange(date) |>
unique() |>
subset(!is.na(temp))
save(meteo_colombo, file = "./data/meteo_colombo.Rda")
conflicted::conflict_scout(
source("./R/00-access-INMET_API-data.R")
)
